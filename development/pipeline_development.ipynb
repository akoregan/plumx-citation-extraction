{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8843ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import dotenv\n",
    "import typing\n",
    "import csv\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceea6e7",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d1b6797",
   "metadata": {
    "tags": [
     "util"
    ]
   },
   "outputs": [],
   "source": [
    "def load_api_credentials ():\n",
    "    \"\"\"Load credentials from .env file.\"\"\"\n",
    "    \n",
    "    dotenv.load_dotenv ()\n",
    "\n",
    "    api_key = os.getenv (\"ELSEVIER_API_KEY\")\n",
    "    inst_token = os.getenv (\"ELSEVIER_INST_TOKEN\", \"\")\n",
    "\n",
    "    return api_key, inst_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70086263",
   "metadata": {
    "tags": [
     "util"
    ]
   },
   "outputs": [],
   "source": [
    "def save_binary_file (filepath, data) :\n",
    "    \"\"\"Save binary content to file.\"\"\"\n",
    "\n",
    "    with open (filepath, \"wb\") as f :\n",
    "        f.write (data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "735e1153",
   "metadata": {
    "tags": [
     "util"
    ]
   },
   "outputs": [],
   "source": [
    "def write_headers (api_key, inst_token, accept = \"*/*\") :\n",
    "    \"\"\"Generate header construction for API call.\"\"\"\n",
    "\n",
    "    headers = { \"X-ELS-APIKey\" : api_key , \"Accept\" : accept}\n",
    "\n",
    "    if inst_token != \"\" :\n",
    "        headers[\"X-ELS-Insttoken\"] = inst_token\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2e7053",
   "metadata": {
    "tags": [
     "util"
    ]
   },
   "outputs": [],
   "source": [
    "def join_with_operator (search_terms: list, operator: typing.Literal[\"AND\", \"OR\"]) -> str:\n",
    "    \"\"\"Generic list joining with operator.\"\"\"\n",
    "\n",
    "    if operator not in [\"AND\", \"OR\"] :\n",
    "        raise ValueError (\"Operator must be 'AND' or 'OR'.\")\n",
    "\n",
    "    return f\" {operator} \".join (search_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227d6a4",
   "metadata": {},
   "source": [
    "Elsevier APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f940bdc1",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def iterate_search_info (data) :\n",
    "    \"\"\"Helper function for search_database: extracts entry metadata and pagination information in case of multi-page ScienceDirect & Scopus search requests.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary data that's returned from search request.\n",
    "\n",
    "    Returns:\n",
    "        tuple: entry metadata (dict), total hits (int), starting entry no. (int), no. entries per page (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    try :\n",
    "\n",
    "        search_results = data['search-results']\n",
    "\n",
    "        entries = search_results.get ('entry')\n",
    "\n",
    "        total = int (search_results.get ('opensearch:totalResults'))\n",
    "        start = int (search_results.get ('opensearch:startIndex'))\n",
    "        per_page = int (search_results.get ('opensearch:itemsPerPage'))\n",
    "        \n",
    "        return entries, total, start, per_page\n",
    "    \n",
    "    except :\n",
    "\n",
    "        print (\"Key error: 'search-results' does not exist in output. See response below.\")\n",
    "        print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c11d4f",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def fetch_data (url, headers, params = None) :\n",
    "    \"\"\"Executes an API request.\n",
    "\n",
    "    Args:\n",
    "        url (str): web address used to access an API\n",
    "        headers (dict): parameters passed to the headers arguments\n",
    "        params (dict, optional): additional query parameters for specifying request. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when the API response content-type is XML. JSON format is required.\n",
    "\n",
    "    Returns:\n",
    "        requests.Response | dict: Returns the raw response object for image or PDF content, or a parsed dictionary for JSON content.\n",
    "    \"\"\"\n",
    "\n",
    "    if params :\n",
    "        response = requests.get (url, headers=headers, params=params)\n",
    "    else :\n",
    "        response = requests.get (url, headers=headers)\n",
    "    \n",
    "    content_type = response.headers.get('content-type', '')\n",
    "\n",
    "    if \"image\" in content_type or \"pdf\" in content_type :\n",
    "        return response\n",
    "    elif \"json\" in content_type :\n",
    "        return response.json ()\n",
    "    elif \"xml\" in content_type :\n",
    "        raise ValueError (\"Output is in XML. Specify 'application/json' in headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edbc00b3",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def write_query (keywords = None, subjs = None, author_ids = None, authors = None, date_range = None) :\n",
    "    \"\"\"Builds a query dictionary with hard-coded start, count, and field keys, an input data range, and \n",
    "    query string generated from optional input parameters. \n",
    "\n",
    "    Args:\n",
    "        keywords (list, optional): list of keywords to search in title fields. Defaults to None.\n",
    "        subjs (list, optional): list of subject areas to filter by. Defaults to None.\n",
    "        author_ids (list, optional): list of author IDs to filter results. Defaults to None.\n",
    "        authors (list, optional): list of author names to search for. Defaults to None.\n",
    "        date_range (str, optional): date range filter in the format expected by the API. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when no search criteria are provided, resulting in an empty query string.\n",
    "\n",
    "    Returns:\n",
    "        dict: A parameters dictionary containing the constructed query string along with pagination \n",
    "        settings (start, count), date range, and requested fields (DOI, title, publication name, \n",
    "        publication date, creator).\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"start\" : 0, \n",
    "        \"count\" : 50,\n",
    "        \"field\" : \"prism:doi,dc:title,prism:publicationName,prism:coverDate,dc:creator\"\n",
    "        }\n",
    "    \n",
    "    if date_range :\n",
    "        params[\"date\"] = date_range\n",
    "\n",
    "    query_list = [] \n",
    "\n",
    "    if keywords :\n",
    "        keywords = [f\"'{keyword}'\" for keyword in keywords]\n",
    "        keywords = join_with_operator (keywords, \"AND\")\n",
    "        query_list.append (f\"TITLE({keywords})\")\n",
    "    if subjs :\n",
    "        subjs = join_with_operator (subjs, \"OR\")\n",
    "        query_list.append (f\"SUBJAREA({subjs})\")\n",
    "    if author_ids :\n",
    "        author_ids = [f\"AU-ID({str(id)})\" for id in author_ids]\n",
    "        author_ids = join_with_operator (author_ids, \"OR\")\n",
    "        query_list.append (author_ids)\n",
    "    if authors :\n",
    "        authors = join_with_operator (authors, \"OR\")\n",
    "        query_list.append (f\"AUTHOR-NAME({authors})\")\n",
    "    \n",
    "    params[\"query\"] = join_with_operator (query_list, \"AND\")\n",
    "\n",
    "    if params[\"query\"] == \"\" :\n",
    "        raise ValueError (\"Query parameter cannot be empty.\")\n",
    "    else :\n",
    "        return params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4c0221",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def filepath_to_output (output_name) : \n",
    "\n",
    "    if IPython.get_ipython () :\n",
    "        __file__ = \"placeholder_for_jupyter.will_not_run_in_py\"\n",
    "    else :\n",
    "        pass\n",
    "\n",
    "    current_dir = os.path.dirname (os.path.abspath (__file__))\n",
    "    output_dir = os.path.join (current_dir, \"..\", \"results\", output_name)\n",
    "\n",
    "    return output_dir  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07a0572",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_count (categories, citation_type, kind):\n",
    "    \"\"\"Searches through the nested structure of the PlumX API response categories to identify citation \n",
    "    counts of specified type & kind.\n",
    "\n",
    "    Args:\n",
    "        categories (list): list of category dictionaries.\n",
    "        citation_type (str): the type of citation to search for.\n",
    "        kind (str): the category name to search for.\n",
    "\n",
    "    Returns:\n",
    "        int: The total count of the specified citation type or 0 if citation type or kind is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    for c in categories:\n",
    "        if c[\"name\"] == kind:\n",
    "            types = c[\"count_types\"]\n",
    "            for t in types:\n",
    "                if citation_type in t[\"name\"].lower():\n",
    "                    return t[\"total\"]\n",
    "                \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dc67c22",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def access_citation_counts (plumx_response, citation_type):\n",
    "    \"\"\"Extracts and returns citation count data from a PlumX response object. Supports querying \n",
    "    for news mentions, policy citations, or both types simultaneously.\n",
    "\n",
    "    Args:\n",
    "        plumx_response (dict): a PlumX API response dictionary.\n",
    "        citation_type (str): the type of citation to retrieve. Must be one of: \"news\", \"policy\", \n",
    "        or \"both\".\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Raised when citation_type is not one of the valid options.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the requested citation count(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    citation_type = citation_type.lower().strip()\n",
    "    \n",
    "    try:\n",
    "        categories = plumx_response[\"count_categories\"]\n",
    "    except:\n",
    "        try:\n",
    "            print(f\"No PlumX data for: {plumx_response['id_value']}\")\n",
    "            \n",
    "            if citation_type == \"both\" :\n",
    "                return {\"news\": None, \"policy\": None}\n",
    "            else :\n",
    "                return {citation_type : None}\n",
    "            \n",
    "        except:\n",
    "            print(\"No PlumX data. Article missing DOI.\")\n",
    "            \n",
    "            if citation_type == \"both\" :\n",
    "                return {\"news\": None, \"policy\": None}\n",
    "            else :\n",
    "                return {citation_type : None}\n",
    "    \n",
    "    if citation_type == \"both\":\n",
    "        news_count = extract_count(categories, \"news\", \"mention\")\n",
    "        policy_count = extract_count(categories, \"policy\", \"citation\")\n",
    "        return {\"news\": news_count, \"policy\": policy_count}\n",
    "    \n",
    "    elif citation_type == \"news\":\n",
    "        kind = \"mention\"\n",
    "    elif citation_type == \"policy\":\n",
    "        kind = \"citation\"\n",
    "    else:\n",
    "        raise ValueError(\"Citation type must be 'news', 'policy', or 'both'.\")\n",
    "    \n",
    "    count = extract_count(categories, citation_type, kind)\n",
    "    \n",
    "    return {citation_type : count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70c3967a",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def search_database (keywords = None,\n",
    "                     date_range = None,\n",
    "                     authors = None, \n",
    "                     subjs = None, \n",
    "                     author_ids = None, \n",
    "                     database_name = \"scidir\", \n",
    "                     max_results = 50,\n",
    "                     save_to_csv = False\n",
    "                     ) :\n",
    "    \"\"\"    \n",
    "    Search the Science Direct or Scopus database within the parameters of keyword and date range inputs.\n",
    "\n",
    "    Args:\n",
    "        keywords (list of strs, optional): search keywords (ex: [\"meta-analysis, depression\"]). Defaults to None.\n",
    "        date_range (str, optional): 4-digit years separated by a dash (ex: \"2020-2025\"). Defaults to None.\n",
    "        authors (list of strs, optional): author names (ex: [\"Abdellasset\", \"Fujii\"] or [\"Abdellasset, W\"]). Defaults to None.\n",
    "        subjs (list of strs, optional): scopus only - 4-letter scopus subject code (ex: [\"MEDI\"]). Defaults to None.\n",
    "        author_ids (list of strs, optional): scopus only - scopus author ID number. Defaults to None.\n",
    "        database_name (str, optional): \"scopus\" or \"sciencedirect\", \"scidir\". Defaults to \"scidir\". \n",
    "        max_results (int, optional): safeguard to prevent exhausting API resources; set to None to return all entries. Defaults to 25.\n",
    "        save_to_csv (bool, optional): saves API request to a csv file if True in addition to default json file. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Each dict includes entry metadata (doi, title, publication name, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    api_key, inst_token = load_api_credentials ()\n",
    "    \n",
    "    database_name = database_name.lower().strip()\n",
    "    if database_name == \"scopus\" :\n",
    "        search_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    elif database_name == \"sciencedirect\" or database_name == \"scidir\" :\n",
    "        search_url = \"https://api.elsevier.com/content/search/sciencedirect\"\n",
    "    \n",
    "    if author_ids and \"sciencedirect\" in search_url :\n",
    "        raise ValueError (\"SciDir does not accept subject or author ID. Provide author name or use Scopus.\")\n",
    "    if subjs and \"sciencedirect\" in search_url :\n",
    "        raise ValueError (\"SciDir does not accept the subject query parameter. Use Scopus.\")\n",
    "\n",
    "    headers = write_headers (api_key, inst_token, accept = \"application/json\")\n",
    "    params = write_query (keywords=keywords, subjs=subjs, author_ids=author_ids, authors=authors, date_range=date_range)\n",
    "    print (params)\n",
    "\n",
    "    all_entries = []\n",
    "\n",
    "    while True :\n",
    "\n",
    "        my_request = fetch_data (search_url, headers = headers, params = params)\n",
    "        entries, total, start, per_page = iterate_search_info (my_request)\n",
    "        \n",
    "        all_entries.extend (entries)\n",
    "\n",
    "        next_start = start + len (entries)\n",
    "        print (f'Fetched {next_start}/{total} entries.')\n",
    "\n",
    "        if next_start >= total : \n",
    "            break\n",
    "        if max_results and len (all_entries) >= max_results :\n",
    "            break\n",
    "        \n",
    "        params['start'] = str (next_start)\n",
    "        time.sleep (5.0)\n",
    "\n",
    "    output_dir = filepath_to_output (\"search_queries\")\n",
    "    os.makedirs (output_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    with open (f\"{output_dir}/{database_name}_search_{timestamp}.json\", \"w\") as f :\n",
    "        json.dump (all_entries, f)\n",
    "\n",
    "    if save_to_csv == True :\n",
    "        \n",
    "        with open (f\"{output_dir}/{database_name}_search_{timestamp}.csv\", \"w\") as f :\n",
    "            writer = csv.writer (f)\n",
    "            writer.writerow (all_entries[0].keys()) # write column headers\n",
    "            writer.writerows([entry.values() for entry in all_entries])\n",
    "\n",
    "    return all_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc546c87",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def get_plumx_metrics (entries, save_to_file = True) :\n",
    "    \"\"\"Accesses PlumX news and policy metrics for a list of input articles, identified via DOI.\n",
    "\n",
    "    Args:\n",
    "        entries (list of dicts): List of entries - each represented by a dict with \"prism:doi\" key.\n",
    "        save_to_file (bool, optional): saves API request to a csv file if True. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts: Equivalent to input parameter entries except with entry-specific PlumX data \n",
    "        appended to each dict.\n",
    "    \"\"\"     \n",
    "\n",
    "    api_key, inst_token = load_api_credentials ()\n",
    "\n",
    "    for i in range (len (entries)) :\n",
    "\n",
    "        try :\n",
    "            article_doi = entries[i][\"prism:doi\"]\n",
    "        except :\n",
    "            print (f\"Article no. {i} missing DOI.\")\n",
    "            entries[i][\"policy_citation_count\"] = None\n",
    "            entries[i][\"news_mentions\"] = None\n",
    "            pass\n",
    "\n",
    "        plumx_url = f\"https://api.elsevier.com/analytics/plumx/doi/{article_doi}\"\n",
    "        headers = write_headers (api_key, inst_token, accept = \"application/json\")\n",
    "        \n",
    "        my_request = fetch_data (plumx_url, headers = headers)\n",
    "\n",
    "        citation_counts = access_citation_counts (my_request, \"both\")\n",
    "        entries[i][\"policy_citation_count\"] = citation_counts[\"policy\"]\n",
    "        entries[i][\"news_mentions\"] = citation_counts[\"news\"]\n",
    "    \n",
    "    entries = [entry for entry in entries if \"policy_citation_count\" in entry and \"news_mentions\" in entry]\n",
    "    \n",
    "    entry_sorter = lambda item: (\n",
    "        item[\"policy_citation_count\"] if item[\"policy_citation_count\"] is not None else float(\"-inf\"), \n",
    "        item[\"news_mentions\"] if item[\"policy_citation_count\"] is not None else float(\"-inf\")\n",
    "    )\n",
    "\n",
    "    entries = sorted (entries, key=entry_sorter, reverse=True)\n",
    "\n",
    "    if save_to_file == True :\n",
    "\n",
    "        output_dir = filepath_to_output (\"search_queries\")\n",
    "        os.makedirs (output_dir, exist_ok=True)\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        with open (f\"{output_dir}/plumx_output_{timestamp}.csv\", \"w\") as f :\n",
    "            writer = csv.writer (f)\n",
    "            writer.writerow (entries[0].keys()) # write column headers\n",
    "            writer.writerows([entry.values() for entry in entries])\n",
    "    \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6e8a26f",
   "metadata": {
    "tags": [
     "service"
    ]
   },
   "outputs": [],
   "source": [
    "def retrieve_article_graphics (article_dictionary, save_manuscript = False) :\n",
    "    \"\"\"Saves all graphics in high-res for a specified article to the input directory. \n",
    "\n",
    "    Args:\n",
    "        article_dictionary (dict): Article metadata dictionary, including the key \"prism:doi\".\n",
    "        save_manuscript (bool, optional): Saves any PDFs associated with article to specified directory. Defaults to False.\n",
    "        filepath (str, optional): Directory in which to store retrieved objects. Defaults to \"./object_retrieval_data\".\n",
    "\n",
    "    Raises:\n",
    "        KeyError: The article dictionary parameter must include the key \"prism:doi\".\n",
    "    \"\"\"\n",
    "    \n",
    "    api_key, inst_token = load_api_credentials ()\n",
    "\n",
    "    article_doi = article_dictionary.get (\"prism:doi\")\n",
    "    if not article_doi :\n",
    "        print (\"DOI not found in article_dictionary.\")\n",
    "        return\n",
    "\n",
    "    try :\n",
    "        \n",
    "        object_retrieval_url = f\"https://api.elsevier.com/content/object/doi/{article_doi}\"\n",
    "        headers = write_headers (api_key, inst_token, accept = \"application/json\")\n",
    "        object_data = fetch_data (object_retrieval_url, headers=headers)\n",
    "\n",
    "    except :\n",
    "\n",
    "        print (f\"Trouble executing article retrieval API for article: {article_doi}\")\n",
    "        return\n",
    "    \n",
    "    try :\n",
    "\n",
    "        object_data = object_data['choices']['choice']\n",
    "        graphics = sorted (list ({obj[\"@ref\"] for obj in object_data if \"gr\" in obj[\"@ref\"]}))\n",
    "    \n",
    "    except :\n",
    "\n",
    "        print (f\"Could not find any graphic renderings in article retrieval response: {article_doi}. May not exist.\")\n",
    "        graphics = None\n",
    "\n",
    "    output_dir = filepath_to_output (\"object_requests\")\n",
    "    graphic_renderings_dir = os.path.join (output_dir, \"graphic_renderings\")\n",
    "    os.makedirs (graphic_renderings_dir, exist_ok=True)\n",
    "    if save_manuscript:\n",
    "        author_manuscripts_dir = os.path.join(output_dir, \"author_manuscripts\")\n",
    "        os.makedirs(author_manuscripts_dir, exist_ok=True)\n",
    "    \n",
    "    headers [\"Accept\"] = \"*/*\"\n",
    "\n",
    "    if graphics :\n",
    "        for gr in graphics :\n",
    "            \n",
    "            try :\n",
    "                object_specified_url = f\"{object_retrieval_url}/ref/{gr}/high\"\n",
    "                object_request = fetch_data (object_specified_url, headers=headers)\n",
    "            \n",
    "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "                a_filepath = os.path.join (graphic_renderings_dir, f\"{article_doi.replace('/','.')}_{gr}_{timestamp}.jpg\")\n",
    "                save_binary_file (a_filepath, object_request)\n",
    "            \n",
    "            except :\n",
    "                print (f\"Could not retrieve {gr} from article: {article_doi}\")\n",
    "                continue\n",
    "\n",
    "    if save_manuscript == True :\n",
    "\n",
    "        try :\n",
    "\n",
    "            man_urls = list ({obj[\"$\"] for obj in object_data if \"am\" in obj[\"@ref\"].lower() and \"pdf\" in obj[\"@type\"].lower()})\n",
    "\n",
    "            if not man_urls :\n",
    "                print (f\"No PDFs found for manuscript: {article_doi}.\")\n",
    "                return\n",
    "        \n",
    "        except :\n",
    "\n",
    "            print (f\"Could not find any author manuscripts in article retrieval response: {article_doi}. May not exist.\")\n",
    "            man_urls = None\n",
    "\n",
    "        if man_urls :\n",
    "            for man_url in man_urls : \n",
    "\n",
    "                try :\n",
    "                        \n",
    "                    pdf_request = fetch_data (man_url, headers = headers)\n",
    "                    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                    a_filepath = os.path.join (author_manuscripts_dir, f\"manuscript_{timestamp}.pdf\")\n",
    "                    save_binary_file (a_filepath, pdf_request)\n",
    "        \n",
    "                except :\n",
    "\n",
    "                    print (f\"Could not download author manuscript for article: {article_doi}\")\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76567506",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69ac0630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 0, 'count': 50, 'field': 'prism:doi,dc:title,prism:publicationName,prism:coverDate,dc:creator', 'query': \"TITLE('meta-analysis')\"}\n",
      "Fetched 50/272189 entries.\n",
      "{'start': 0, 'count': 50, 'field': 'prism:doi,dc:title,prism:publicationName,prism:coverDate,dc:creator', 'query': \"TITLE('meta-analysis')\"}\n",
      "Fetched 50/81945 entries.\n"
     ]
    }
   ],
   "source": [
    "# test search database function\n",
    "\n",
    "search_returns_scopus = search_database (database_name = \"scopus\",\n",
    "                                  keywords = [\"meta-analysis\"],\n",
    "                                  save_to_csv = True)\n",
    "\n",
    "search_returns_scidir = search_database (keywords = [\"meta-analysis\"],\n",
    "                                  save_to_csv = True) # default database is sciencedirect\n",
    "\n",
    "\n",
    "# end search database test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a3b0931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PlumX data for: 10.12307/2026.649\n",
      "No PlumX data for: 10.12307/2026.597\n",
      "No PlumX data for: 10.1016/j.jormas.2025.102606\n",
      "No PlumX data for: 10.1007/s42832-025-0367-2\n"
     ]
    }
   ],
   "source": [
    "# test plumx metrics function\n",
    "\n",
    "metrics = get_plumx_metrics (search_returns_scopus)\n",
    "\n",
    "# end plumx metrics test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find any graphic renderings in article retrieval response: 10.17305/bb.2025.12979. May not exist.\n",
      "Could not find any author manuscripts in article retrieval response: 10.17305/bb.2025.12979. May not exist.\n"
     ]
    }
   ],
   "source": [
    "# test article retrieval function\n",
    "\n",
    "retrieve_article_graphics (article_dictionary = {\"prism:doi\" : \"doi.org/10.1016/j.ypmed.2022.107130\"}, \n",
    "                           save_manuscript = True)\n",
    "\n",
    "# end article retrieval testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mep_docling_1017",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
